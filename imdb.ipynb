{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential, model_from_json, load_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Dropout, Conv1D, MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(path='imdb.npz',num_words=top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The Dataset has been preprocessed, read: https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Simple LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_model = False\n",
    "if (train_model):\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64, verbose=0, callbacks=[TQDMNotebookCallback(leave_inner=False, leave_outer=True)])\n",
    "    model.save('models/model1.h5')\n",
    "else:\n",
    "    model = load_model('models/model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.46%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imdb_word_index = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4454, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_word_index['dinosaur'], imdb_word_index['to']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imdb_words = {}\n",
    "imdb_words[0] = ''\n",
    "for key, value in imdb_word_index.items():\n",
    "    value = int(value)\n",
    "    imdb_words[value] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('dinosaur', 'to')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_words[4454], imdb_words[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vector_to_sentence(vec):\n",
    "    vec = vec.flatten()\n",
    "    sentence = ''\n",
    "    for index in vec:\n",
    "        word = imdb_words[index]+' '\n",
    "        sentence += word\n",
    "    return sentence.strip()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the this enough and and better executed ability br and with his her and movie it stick politics i i was one is excellent cut this and only natural with lot br of how truly full this of want f br and pop and off that however of here br and and me will her points violent this and of 1 for from me in and of guy to simple or and this seen money and sees hours is exciting and guy to sets and and ability his is displays really me of truly and br and and with is quite touches in of i've been stick politics i i of she's it for of house and for its great does it of complicated down roth cute dies are and are arrived really disaster and rented movie trite of thought group fun to big was can about previous are and to and historical those are for top and with is word bath this of neil whether and to audience and with table and who private and yes br complicated and was let musicals was two that with and happen can't ok in out imagination i i they by my complicated other moore is am and place br of problem 1 and movie table and\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_to_sentence(X_train[90])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: LSTM with Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction of Dropout layers to reduce overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(LSTM(100))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_model = False\n",
    "if (train_model):\n",
    "    model2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64, verbose=0, callbacks=[TQDMNotebookCallback(leave_inner=True, leave_outer=True)])\n",
    "    model2.save('models/model2.h5')\n",
    "else:\n",
    "    model2 = load_model('models/model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.35%\n"
     ]
    }
   ],
   "source": [
    "scores = model2.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convergence to the minima is slow because of the dropout, Model 2 will outperform Model 1 on increasing epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: LSTM and CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN is efficient in detecting patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 500, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 216,405\n",
      "Trainable params: 216,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model3.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model3.add(MaxPooling1D(pool_size=2))\n",
    "model3.add(LSTM(100))\n",
    "model3.add(Dense(1, activation='sigmoid'))\n",
    "model3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_model = True\n",
    "if (train_model):\n",
    "    model3.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=64, verbose=0, callbacks=[TQDMNotebookCallback(leave_inner=True, leave_outer=True)])\n",
    "    model3.save('models/model3.h5')\n",
    "else:\n",
    "    model3 = load_model('models/model3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.35%\n"
     ]
    }
   ],
   "source": [
    "scores = model2.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "widgets": {
   "state": {
    "05f7f1c356544bcd888628796f0771ef": {
     "views": [
      {
       "cell_index": 19
      }
     ]
    },
    "18a79b41df264600a5f7a1d6113d498b": {
     "views": [
      {
       "cell_index": 25
      }
     ]
    },
    "800503c676ab4e1dbf3e247abfd444c8": {
     "views": [
      {
       "cell_index": 19
      }
     ]
    },
    "8183d1aaf3b54183b6337fb1d1cb3b0b": {
     "views": [
      {
       "cell_index": 25
      }
     ]
    },
    "91cad089746844b493c5d88d214c2d5c": {
     "views": [
      {
       "cell_index": 19
      }
     ]
    },
    "98965a94f3bb4d3eb3d341fcb172d15b": {
     "views": [
      {
       "cell_index": 19
      }
     ]
    },
    "b7b51dd6d6914ed0a78b4b5ae1f66f9f": {
     "views": [
      {
       "cell_index": 19
      }
     ]
    },
    "cab3fd68d3304bff9ead0d9928372dfb": {
     "views": [
      {
       "cell_index": 25
      }
     ]
    },
    "cd2206fa00754969bed258eea057627b": {
     "views": [
      {
       "cell_index": 25
      }
     ]
    },
    "db77e9d808f04d739140d97fcd37ea77": {
     "views": [
      {
       "cell_index": 25
      }
     ]
    },
    "ecdbda9230974bb484c0d193e611d102": {
     "views": [
      {
       "cell_index": 19
      }
     ]
    },
    "f13f3a7e7f3b4f4ebbaf4ae876f38c6a": {
     "views": [
      {
       "cell_index": 25
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
